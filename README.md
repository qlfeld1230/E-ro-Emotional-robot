# **E-ro: ê°ì •ì¸ì‹ ë¡œë´‡ ğŸ¤–**  
**E-ro: Emotional Recognition Robot**

**E-ro**ëŠ” ì‚¬ëŒì˜ ê°ì •ì„ ì¸ì‹í•˜ê³  ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ê°ì •ì¸ì‹ ë¡œë´‡ìœ¼ë¡œ, **2023 ì°½ì˜ì  ì¢…í•©ì„¤ê³„ ì»¨ì†Œì‹œì—„** í”„ë¡œì íŠ¸ì˜ ì¼í™˜ìœ¼ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë¡œë´‡ì€ **ë©”ë¼ë¹„ì–¸ì˜ ë²•ì¹™**(í‘œì • 55%, ìŒìƒ‰ 38%, ë‹¨ì–´ 7%)ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì •ì„ ë¶„ì„í•˜ë©°, **ì¹´ë©”ë¼**, **ìŒì„± ë¶„ì„**, **ë¬¸ì¥ ë¶„ì„** ê¸°ìˆ ì„ ê²°í•©í•´ ì •ë°€í•œ ê°ì • ì¸ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.  
**E-ro** is an emotional recognition robot designed to recognize and interact with human emotions. Developed as part of the **2023 Creative Comprehensive Design Consortium**, this robot uses **Mehrabian's Rule** (55% facial expression, 38% tone, 7% words) to analyze emotions by combining facial, vocal, and text-based sentiment analysis.

---

## **í”„ë¡œì íŠ¸ ê°œìš” / Project Overview**

E-roëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ìˆ ê³¼ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:  
E-ro utilizes the following technologies and approaches:

- **í‘œì • ë¶„ì„ (Facial Analysis)**: 28,709ê°œì˜ ì–¼êµ´ ë°ì´í„°ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ í•™ìŠµí•œ ê°ì • ì¸ì‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ í‘œì •ì„ ë¶„ì„í•©ë‹ˆë‹¤.  
  **Emotion recognition using facial expressions** trained on 28,709 facial datasets using machine learning.

- **ìŒìƒ‰ ë¶„ì„ (Vocal Tone Analysis)**: 28ëª…ì˜ í™”ìë¡œë¶€í„° 2,000ê°œì˜ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ëª¨ë¸ë¡œ ìŒì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.  
  **Speech tone analysis using audio data** trained from 28 speakers (2,000 audio files).

- **ë¬¸ì¥ ë¶„ì„ (Sentence Analysis)**: Google Speech-to-Text(STT)ì™€ Text-to-Speech(TTS) APIë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì¥ì˜ ê°ì •ì„ ì¸ì‹í•©ë‹ˆë‹¤.  
  **Text-based emotion recognition** using Google Speech-to-Text (STT) and Text-to-Speech (TTS) APIs.

- **ë©”ë¼ë¹„ì–¸ì˜ ë²•ì¹™ (Mehrabian's Rule)**: í‘œì •(55%), ìŒìƒ‰(38%), ë‹¨ì–´(7%)ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ê°ì •ì„ íŒë‹¨í•©ë‹ˆë‹¤.  
  **Emotion judgment based on Mehrabian's Rule**: Combining facial expressions (55%), vocal tone (38%), and words (7%).

---

## **ì£¼ìš” ê¸°ëŠ¥ / Key Features**

### 1. **í‘œì • ê°ì • ì¸ì‹ / Facial Emotion Recognition**
- **ëª¨ë¸ ì¶œì²˜ / Model Source**: [Emotion Recognition by omar-aymen](https://github.com/omar-aymen/Emotion-recognition)  
- **ë™ì‘ ë°©ì‹ / Functionality**:
  - ì–¼êµ´ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ë©° ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤.  
    Real-time tracking and analysis of facial expressions.
  - ê°ì • ê°’ì´ 40 ì´ìƒì¸ ìƒíƒœê°€ 2ì´ˆ ì´ìƒ ìœ ì§€ë  ê²½ìš° ìœ íš¨ ë°ì´í„°ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.  
    If an emotion score remains above 40 for more than 2 seconds, it is considered valid.

### 2. **ì–¼êµ´ ì¶”ì  / Facial Tracking**
- **ëª¨ë¸ ì¶œì²˜ / Model Source**: [Face Tracking with Raspberry Pi](https://core-electronics.com.au/guides/Face-Tracking-Raspberry-Pi/#Open)  
- **ê¸°ëŠ¥ / Features**:
  - ì‹¤ì‹œê°„ìœ¼ë¡œ ì–¼êµ´ì˜ ìœ„ì¹˜ë¥¼ ì¶”ì í•˜ê³  ì¹´ë©”ë¼ì˜ ë°©í–¥ì„ ì¡°ì •í•©ë‹ˆë‹¤.  
    Tracks the position of the face in real time and adjusts the camera's direction.

### 3. **ìŒìƒ‰ ê°ì • ì¸ì‹ / Vocal Tone Analysis**
- **ëª¨ë¸ ì¶œì²˜ / Model Source**: [Speech Emotion Analyzer by MiteshPuthran](https://github.com/MiteshPuthran/Speech-Emotion-Analyzer)  
- **ê¸°ëŠ¥ / Features**:
  - ìŒì„±ì˜ ì–µì–‘, ì†ë„, ìŒë†’ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤.  
    Analyzes tone, speed, and pitch of the voice to determine emotion.
  - ì‚¬ìš©ìì˜ ìŒì„± ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.  
    Processes user speech data in real time.

### 4. **ë¬¸ì¥ ê°ì • ì¸ì‹ / Sentence Emotion Recognition**
- **API ì¶œì²˜ / API Source**: Google Speech-to-Text ë° Text-to-Speech API  
- **ê¸°ëŠ¥ / Features**:
  - ë¬¸ì¥ì—ì„œ ê°ì • íŒë‹¨ì˜ ê¸°ì¤€ì´ ë˜ëŠ” ë‹¨ì–´ì™€ ë¶€ì •ì–´ ì¡°í•©ì„ ë¶„ì„í•©ë‹ˆë‹¤.  
    Identifies emotional cues and negations in sentences to determine emotion.
  - í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ê¸ì •, ë¶€ì •, ì¤‘ë¦½ ê°ì •ì„ íŒë‹¨í•©ë‹ˆë‹¤.  
    Determines positive, negative, or neutral sentiments using text analysis.

### 5. **ë©”ë¼ë¹„ì–¸ì˜ ë²•ì¹™ ê¸°ë°˜ ê°ì • íŒë‹¨ / Emotion Judgment via Mehrabian's Rule**
- í‘œì •(55%), ìŒìƒ‰(38%), ë‹¨ì–´(7%) ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ê°ì •ì„ íŒë‹¨í•©ë‹ˆë‹¤.  
  Final emotion is determined by applying weights: facial expressions (55%), vocal tone (38%), and words (7%).

---

## **ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ / System Requirements**

### **í•˜ë“œì›¨ì–´ / Hardware**
- **Raspberry Pi 4** or similar microcontroller  
- **Camera module** (for facial emotion recognition and tracking)  
- **Microphone** (for voice input)  
- **Speaker** (for voice output)  
- **Servo motors** (for motion control)  
- Additional: Touch sensor (for tactile interaction)  

### **ì†Œí”„íŠ¸ì›¨ì–´ / Software**
- **Python 3.8 ì´ìƒ / Python 3.8+**  
- í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ / Required Libraries:
  - **OpenCV**: ì–¼êµ´ ê°ì§€ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ / Facial detection and image processing
  - **TensorFlow**: ê°ì • ë¶„ì„ ëª¨ë¸ / Emotion analysis model
  - **SpeechRecognition**: ìŒì„± ì¸ì‹ / Speech recognition
  - **librosa**: ìŒì„± ì‹ í˜¸ ë¶„ì„ / Speech signal processing
  - **Google Cloud Speech & TTS API** ì„¤ì • í•„ìš” / Requires Google Cloud Speech & TTS API setup

---

## **GPIO pin diagram**

